<!DOCTYPE html>
<html lang="es-AR">

<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="author" content="Lucia Driussi" />
<meta name="keywords" content="IA,historia,inteligencia artificial,informativo,educativo">
<meta name="description" content="IA, AI,inteligencia artificial,breve historia">
  <link rel="stylesheet" href="Estilos-Lucia.css" />
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="stylesheet"
    href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200&icon_names=home" />
  <link
    href="https://fonts.googleapis.com/css2?family=Abel&family=Allerta+Stencil&family=Alumni+Sans+Pinstripe:ital@0;1&family=Asap:ital,wght@0,100..900;1,100..900&family=Audiowide&family=Faculty+Glyphic&family=Lexend+Mega:wght@100..900&family=Miss+Fajardose&family=Montserrat+Alternates:ital,wght@0,100;0,200;0,300;0,400;0,500;0,600;0,700;0,800;0,900;1,100;1,200;1,300;1,400;1,500;1,600;1,700;1,800;1,900&family=Quattrocento+Sans:ital,wght@0,400;0,700;1,400;1,700&family=REM:ital,wght@0,100..900;1,100..900&family=Special+Elite&family=Spectral+SC:ital,wght@0,200;0,300;0,400;0,500;0,600;0,700;0,800;1,200;1,300;1,400;1,500;1,600;1,700;1,800&family=Syne+Mono&family=Voces&display=swap"
    rel="stylesheet">
  <title>Modelos Antiguos de Inteligencia Artificial</title>
</head>

<body class="fuente">
  <div class="contenedortitulo fuentetitulo">
    <header>
      <h1>Breve Historia de la Inteligencia Artificial</h1>
    </header>
  </div>
  <nav class="menudiv">
    <div class="boton">
      <button><a href="#">Inicio</a></button>
      <button> <a href="#">IA: Conceptos y Diferencias</a></button>
      <button>
        <a href="#">Aplicaciones IA en Desarrollo Web</a> </button>
      <button>
        <a href="#">Ética y sesgos de la IA</a><button>
       <button>
        <a href="#">Herramientas y APIs de IA</a><button>
    </div>
  </nav>
  <main>
    <div class="sidebar">
      <h3>CONTENIDOS <img src="fotos-pag/svg/cognition_AntiqueWhite.svg" alt=""></h3>
      <nav>
        <div class="menu-botones sidebar fuentetitulo">
          <a href="#Parte1" class="btn botones">1950</a>
          <a href="#Parte2" class="btn botones">1961</a>
          <a href="#Parte3" class="btn botones">1966</a>
          <a href="#Parte4" class="btn botones">1970</a>
          <a href="#Parte5" class="btn botones">1974</a>
          <a href="#Parte6" class="btn botones">1980</a>
          <a href="#Parte7" class="btn botones">1987</a>
          <a href="#Parte8" class="btn botones">2007</a>
          <a href="#Parte9" class="btn botones">2012</a>
          <a href="#Parte10" class="btn botones">2020</a>
        </div>
      </nav>
    </div>
    <section class="contenido">
      <h1 id="Parte1">1950 <img src="fotos-pag/svg/line_start_circle_24dp_000000_FILL0_wght400_GRAD0_opsz24.svg" alt="">
        Las Contribuciones Fundamentales de Alan Turing</h1>
      <br>
      <p>
        Más allá de su biografía, el legado de Alan Turing reside en pilares conceptuales que dieron forma a la era
        digital:
        <img src="fotos-pag/turing.jpg" alt="Turing" height="250px" width="200px" class="bordeimagen" align="right"
          title="Alan Turing.">
        <br>
        <br>
        <img src="fotos-pag/svg/neurology_24dp_000000_FILL0_wght400_GRAD0_opsz24.svg" alt=""> <b>La Máquina de Turing
          (1936):</b> Este no fue un dispositivo físico, sino un modelo teórico revolucionario.
        Turing
        imaginó una máquina abstracta que manipulaba símbolos en una cinta siguiendo un conjunto de reglas. Este
        concepto
        simple pero poderoso definió formalmente qué es un "algoritmo" y qué significa "computable". Se convirtió en la
        base teórica de todas las computadoras modernas, estableciendo los límites de lo que puede y no puede ser
        calculado por una máquina.
        <br>
        <img src="fotos-pag/svg/neurology_24dp_000000_FILL0_wght400_GRAD0_opsz24.svg" alt=""> <b>El Test de Turing
          (1950):</b> En su artículo "Computing Machinery and Intelligence", Turing abordó la
        pregunta
        "¿Pueden pensar las máquinas?" de una manera práctica. Propuso el "Juego de la Imitación": si un interrogador
        humano, al conversar con una máquina y una persona a través de un terminal, no puede distinguir cuál es cuál,
        entonces la máquina puede ser considerada inteligente. Este criterio, conocido como el Test de Turing, se
        convirtió en el objetivo fundacional y el estándar de referencia para el campo de la Inteligencia Artificial.
        <br>
        Podemos decir ,en efecto,que proporcionó el marco teórico y las preguntas correctas que hicieron posible la
        informática y la inteligencia artificial. Su mente visionaria pasó de resolver los problemas
        matemáticos más abstractos a aplicar esa lógica para descifrar códigos enemigos y, finalmente, para preguntarse
        sobre el futuro de la inteligencia misma.
      </p>
      <br>
      <h1 id="Parte2">1961 <img src="fotos-pag/svg/line_start_circle_24dp_000000_FILL0_wght400_GRAD0_opsz24.svg" alt="">
        ELIZA</h1>
      <br>
      <p>
      <figure><img src="fotos-pag/eliza_mj_illustration_retro_computer.png" alt="" height="250px" width="350px" alt=""
          class="bordeimagen" align="left"
          title="Imagen ilustrativa de como se proyectaba ELIZA en los ordenadores de la epoca"></figure>
      <b>ELIZA</b> es el primer programa informático de procesamiento del
      lenguaje natural creado entre 1964 y 1966,en el MIT por Joseph Weizenbam. <br>
      Creada para explorar la comunicación entre
      humanos y máquinas,ELIZA simuló la conversación utilizando una
      metodología de <em>concordancia y sustitución de patrones</em> que daba a los
      usuarios una ilusión de entendimiento por parte del programa, pero
      no tenía ninguna representación que pudiera considerarse que
      realmente comprendía lo que se decía por ninguna de las dos
      partes. El guion más famoso, <em>DOCTOR</em>,
      simulaba un psicoterapeuta de la escuela rogeriana (en la que el
      terapeuta a menudo refleja las palabras del paciente al
      paciente),y utilizaba reglas, dictadas en el guion, para
      responder con preguntas no direccionales en las entradas de los
      usuarios.
      <br>
      Como tal, ELIZA <b>fue uno de los primeros chatterbots</b>
      ("chatbot" modernamente) y uno de los primeros programas capaces de
      ejecutar la prueba de Turing. El creador de ELIZA,<em>Weizenbaum</em>,
      pensaba que el programa podía ser un método para explorar la
      comunicación entre humanos y máquinas. Muchos académicos
      creían que el programa podría influir positivamente en la vida de
      muchas personas, especialmente aquellas con problemas psicológicos,
      y que podría ayudar a los médicos que trabajan en el tratamiento de
      estos pacientes. Aunque ELIZA era capaz de participar en el
      discurso, no podía conversar con la verdadera comprensión. Sin
      embargo, muchos usuarios tempranos estaban convencidos de la
      inteligencia y la comprensión de ELIZA, a pesar de la insistencia de
      Weizenbaum en lo contrario. El código fuente original de ELIZA <b>había desaparecido desde su creación en la
        década de 1960</b>, ya que no
      era habitual publicar artículos que incluyeran código fuente en ese
      momento. Sin embargo, más recientemente, el código fuente MAD-SLIP
      se ha descubierto en los archivos del MIT y se ha publicado en
      diversas plataformas, como archive.org. El código fuente tiene un
      gran interés histórico, ya que demuestra no sólo la especificidad de
      los lenguajes y técnicas de programación en ese momento, sino
      también el inicio de la abstracción y la abstracción de software
      como medio para conseguir una programación de software sofisticada.
      <br>
      <b>A continuación una simulación del código de ELIZA:</b>
      <br>
      <br>
      <div class="codigo-backround">
        <pre><code>function ELIZA GENERATOR(user sentence) returns response
   Let w be the word in sentence that has the highest keyword rank
   if w exists
       Let r be the highest ranked rule for w that matches sentence
       response ← Apply the transform in r to sentence
       if w = 'my'
           future ← Apply a transformation from the ‘memory’ rule list to sentence
           Push future onto the memory queue
       else (no keyword applies)
           Either
               response ← Apply the transform for the NONE keyword to sentence
           Or
               response ← Pop the oldest response from the memory queue
   Return response</code></pre>
      </div>
      </p>
      <br>
      <h1 id="Parte3">1966 <img src="fotos-pag/svg/line_start_circle_24dp_000000_FILL0_wght400_GRAD0_opsz24.svg" alt="">
        SHAKEY</h1>
      <br>
      <img src="fotos-pag/SHAKEY.webp" alt="" height="250px" width="350px" alt="" class="bordeimagen" align="left">
      <p>Desarrollado en el <em> Stanford Research Institute</em>, <b>Shakey</b> fue el <b>primer robot móvil que podía
          percibir su entorno,
          razonar sobre sus acciones y planificar rutas.</b>A diferencia de robots anteriores que seguían patrones
        preprogramados,Shakey utilizaba lógica para tomar decisiones. Podía analizar comandos como "empuja el bloque de
        la plataforma" y descomponerlo en sub-tareas ejecutables. Este pionero <b>sentó las bases para la robótica
          moderna
          y la planificación automática.</b></p>
      <br>
      <br>
      <h1 id="Parte4">1970 <img src="fotos-pag/svg/line_start_circle_24dp_000000_FILL0_wght400_GRAD0_opsz24.svg" alt="">
        MYCIN</h1>
      <br>
      <p>
        Especializado en diagnóstico de enfermedades infecciosas de la sangre, este sistema <b>operaba con 500 reglas
          médicas y alcanzaba una precisión del 69%, comparable al 80% de expertos humanos.</b>En pleno primer invierno
        de la
        IA,<b>MYCIN</b> demostró el potencial práctico de los sistemas expertos. Lo más innovador era su
        <b> capacidad para explicar su razonamiento, mostrando la cadena lógica que llevaba a cada diagnóstico.</b>
        Aunque
        nunca se implementó clínicamente por preocupaciones legales, revolucionó el enfoque de la <b>IA aplicada a
          medicina.</b>
      </p>
      <br>
      <h1 id="Parte5">1974-1993 <img src="fotos-pag/svg/line_start_circle_24dp_000000_FILL0_wght400_GRAD0_opsz24.svg"
          alt=""> Invierno de las I.A.</h1>
      <br>
      <p>El primer invierno (1974-1980) <b>surgió cuando sistemas como el Traductor Automático fallaron estrepitosamente,
        llevando a recortes drásticos en financiamiento. </b>
      <br>
      Fueron períodos de estancamiento causado por expectativas irreales que chocan con las limitaciones técnicas, lo que provoca una drástica reducción de la financiación y el interés tras los primeros avances de la lógica simbólica y el fracaso comercial de los sistemas expertos. Hoy, con el boom de la IA generativa, muchos especulan con un nuevo invierno. Los motivos son la posible saturación del mercado, las promesas exageradas de una inteligencia a nivel humano, los altos costes energéticos y las limitaciones actuales de los modelos, como las "alucinaciones" o los sesgos.En esencia, es el ciclo recurrente del entusiasmo desmedido que precede a la desilusión.
</p>
      <br>
      <h1 id="Parte6">1980 <img src="fotos-pag/svg/line_start_circle_24dp_000000_FILL0_wght400_GRAD0_opsz24.svg" alt="">
        DeepBlue</h1>
      <div align="center">
        <img src="fotos-pag/1zBg15XiJ-Yo6G8q_BXgwsQ-1.webp" class="bordeimagen" alt="" height="350px" width="550px" title="La victoria de Deep Blue en Ajedrez">
      </div>
      <p>Emergiendo después del periodo de invierno de las IA , <b>Deep Blue</b> marcó un renacimiento simbólico para ellas.
        Cuando venció al campeón mundial Garry Kasparov en 1997, estasupercomputadora de IBM  <b> podía evaluar 200 millones de posiciones
        por segundo. Lo más significativo fue su uso de hardware especializado y algoritmos que balanceaban búsqueda
        exhaustiva con evaluación posicional.</b> Deep Blue demostró que las máquinas podían superar a humanos en dominios
        que requerían pensamiento estratégico profundo.</p>
      <br>
      <h1 id="Parte7">1987 <img src="fotos-pag/svg/line_start_circle_24dp_000000_FILL0_wght400_GRAD0_opsz24.svg" alt="">
        NETtalk</h1>
      <p>
        <b>NETtalk</b> fue una red neuronal pionera que aprendió a convertir texto en habla. Lo notable era que <b>aprendía
        mediante ejemplos, mejorando gradualmente su pronunciación hasta alcanzar 95% de precisión.</b> 
        <br>
        Demostró que las redes neuronales podían adquirir habilidades complejas mediante aprendizaje, no solo programación explícita,
        anticipando el enfoque del aprendizaje profundo moderno.
        <br>
        A continuación podemos observar una de las primeras pruebas de este modelo:
      </p>
      <br>
<div align="center">
    <a href="https://www.youtube.com/watch?v=gakJlr3GecE" target="_blank">
  <img src="fotos-pag/NetTalk_50.webp" alt="Ver video en YouTube" class="video-thumbnail"></a>
      </div>
      <br>
      <h1 id="Parte8">2007 <img src="fotos-pag/svg/line_start_circle_24dp_000000_FILL0_wght400_GRAD0_opsz24.svg" alt="" >
        IBM Watson</h1>
      <img src="fotos-pag/IBM_Watson.jpg" alt="" class="bordeimagen" alt="" height="180px" width="300px" align="left">
      <br>
      <p><b>IBM Watson</b> originalmente se pensó en mediados de 2007, pero surge finalmente en 2011 no como otro sistema de IA
        especializado, sino como una demostración pública sin
        precedentes de comprensión del lenguaje natural. Su desarrollo, iniciado en 2006 por IBM, tenía un objetivo
        aparentemente simple pero técnicamente revolucionario: competir y vencer en <em>Jeopardy!</em>, un programa donde las
        preguntas están cargadas de sarcasmo, juegos de palabras y referencias culturales que hasta entonces se
        consideraban exclusivas de la comprensión humana.
        <br>
        La arquitectura de Watson, basada en la plataforma <em>DeepQA</em>, representaba un salto cualitativo respecto a sistemas
        anteriores. Mientras los motores de búsqueda tradicionales se limitaban a encontrar documentos relevantes usando
        palabras clave, Watson <b> ejecutaba un sofisticado proceso en cuatro etapas: primero, descomponía la pregunta
        analizando su estructura lingüística completa; segundo, generaba múltiples hipótesis de respuesta consultando
        simultáneamente millones de fuentes; tercero, evaluaba la evidencia para cada hipótesis calculando niveles de
        confianza; y finalmente, sintetizaba la respuesta más precisa.</b> Todo esto ocurría en segundos, gracias a 2.880
        núcleos de procesamiento analizando en paralelo el equivalente a 200 millones de páginas de información.
        <br>
        Lo que realmente diferenciaba a Watson de otros sistemas era su enfoque único. A diferencia de los sistemas
        expertos de los 80, que operaban con reglas predefinidas, <b>aprendía dinámicamente de corpus documentales
        masivos. </b>Y a diferencia de los actuales modelos de lenguaje como <em>GPT</em>, que generan respuestas estadísticamente
        plausibles, Watson fundamentaba cada respuesta en evidencias específicas y medibles, priorizando la precisión
        factual sobre la fluidez conversacional.
        <br>
        La importancia histórica de Watson trasciende a su victoria televisiva. Debido a que <b> demostró que las máquinas podían
        comprender el lenguaje humano en toda su complejidad</b>, sentando las bases técnicas para aplicaciones prácticas en
        medicina - donde analiza historiales médicos y literatura científica para asistir en diagnósticos, finanzas e
        investigación científica. Watson representa <b> el puente crucial entre la IA simbólica del siglo XX y el machine
        learning moderno</b>, recordándonos que la verdadera inteligencia artificial no se trata solo de procesar texto,
        sino de comprender significado y fundamentar el conocimiento.
      </p>
      <br>
      <h1 id="Parte9">2012 <img src="fotos-pag/svg/line_start_circle_24dp_000000_FILL0_wght400_GRAD0_opsz24.svg" alt=""
          > AlexNet</h1>
      <br>
      <p>La arquitectura de <b>AlexNet</b> introdujo innovaciones fundamentales que definirían la próxima década de
        investigación en IA. Con sus ocho capas profundas (cinco convolucionales y tres fully-connected), demostró por
        primera vez que las redes neuronales convolucionales profundas podían escalarse efectivamente. Pero lo
        verdaderamente revolucionario fue cómo combinó tres elementos clave: redes neuronales convolucionales profundas,
        unidades de procesamiento gráfico (GPUs) y el dataset masivo de ImageNet. Las GPUs, tradicionalmente usadas para
        renderizar videojuegos, permitieron entrenar esta red monstruosa en tiempos razonables, procesando los 1.2
        millones de imágenes de ImageNet con sus 1,000 categorías diferentes.
        <br>
        Técnicamente,<b>incorporó avances cruciales como la función de activación ReLU</b>, que aceleró dramáticamente
        el entrenamiento al evitar el problema del vanishing gradient; dropout para prevenir overfitting regularizando
        el modelo; y augmentación de datos para expandir artificialmente el dataset de entrenamiento. Su diseño dual en
        dos GPUs GTX 580 no fue solo una necesidad práctica, sino que demostró que el entrenamiento distribuido era
        viable para modelos de deep learning.
        <br>
       Este modelo cambió el paradigma completo de la visión por computadora, desplazando los métodos tradicionales basados en características
        hand-crafted como SIFT y HOG hacia el <b>aprendizaje end-to-end.</b> Probó que las redes profundas podían aprender
        representaciones jerárquicas automáticamente desde los datos, desde bordes simples en las primeras capas hasta
        características complejas específicas de objetos en capas profundas.
        <br>
        El legado de AlexNet es inmenso: <b> inspiró una explosión de arquitecturas profundas como VGG, GoogLeNet y ResNet;
        democratizó el deep learning al mostrar que las GPUs comerciales podían usarse para investigación seria; y
        estableció el template para la investigación moderna en IA.</b>
      </p>
      <br>
      <h1 id="Parte10">2020-Actualidad <img
          src="fotos-pag/svg/line_start_circle_24dp_000000_FILL0_wght400_GRAD0_opsz24.svg" alt=""> AlphaFold 2</h1>
      <br>
      <p>En 2020, DeepMind presentó <b>AlphaFold 2</b>, un sistema que resolvería uno de los grandes misterios de la biología
        moderna: predecir la estructura tridimensional de las proteínas a partir de su secuencia genética. Este no era
        un problema computacional más, sino un desafío que llevaba 50 años desafiando a científicos en todo el mundo,
        con profundas implicaciones para la medicina y la comprensión de la vida misma.
<br>
        La genialidad de AlphaFold 2 reside en su arquitectura única, que <b>combina redes neuronales con un profundo
        entendimiento biofísico. A diferencia de aproximaciones anteriores, el sistema utiliza un mecanismo de atención
        evolutiva que analiza cómo han co-evolucionado diferentes partes de las proteínas a través de especies,
        revelando pistas sobre su estructura espacial.</b>  Pero su verdadera innovación es el <b>módulo de geometría
        tridimensional</b>, que construye directamente modelos atómicos precisos mediante transformaciones espaciales
        iterativas, como si armara un rompecabezas molecular en tiempo real.
        <br>
      <div align="center">
        <img src="fotos-pag/AlphaFold_2.png" alt="" height="350px" width="550px" class="bordeimagen" title="AlphaFold2 predice la estructura 3D de una proteína.">
      </div>
      Lo que hace excepcional este modelo es su impacto tangible e inmediato. Mientras muchos sistemas de IA se miden
      por métricas abstractas, AlphaFold 2 demostró una precisión superior al 90% en predicciones estructurales,
      igualando o superando métodos experimentales que requieren meses de trabajo y equipos costosos. Su base de datos
      pública, con estructuras predichas para más de 200 millones de proteínas, ha creado un recurso sin precedentes
      para la comunidad científica global.
      <br>
      El verdadero legado de AlphaFold 2 trasciende lo técnico. <b>Está acelerando la investigación de enfermedades como el
      Alzheimer y el Parkinson, facilitando el diseño de enzimas para combatir la contaminación, y revolucionando el
      desarrollo de medicamentos al revelar dianas terapéuticas antes invisibles.</b> Más importante aún, ha <b>establecido un
      nuevo paradigma de colaboración entre la IA y la ciencia fundamental, demostrando que las máquinas pueden ser
      socias en la expansión del conocimiento humano.</b>
<br>
      En el panorama actual de inteligencia artificial, AlphaFold 2 representa la culminación de lo que la tecnología
      puede lograr: no solo resolver problemas complejos, sino abrir nuevas fronteras para el entendimiento humano y el
      avance del bienestar global.</p>
<br>
      <ul class="contenedor-referencias">
        <h3><ins>Refencias/Bibliografía</ins></h3>
        <li><a href="https://www.cultura.gob.ar/alan-turing-el-padre-de-la-inteligencia-artificial-9162/">cultura.gob.ar-Alan Turing el padre de la inteligencia artificial</a></li>
        <li><a href="https://es.wikipedia.org/wiki/ELIZA">wikipedia.org- ELIZA</a></li>
        <li><a href="https://es.wikipedia.org/wiki/Invierno_IA">wikipedia.org- Invierno IA</a></li>
        <li><a
            href="https://es.wikipedia.org/wiki/Historia_de_la_inteligencia_artificial"><em>wikipedia.org-Historia de la inteligencia artificial</em></a>
        </li>
        <li><a href="https://www.ibm.com/new/product-blog/from-checkers-to-chess-a-brief-history-of-ibm-ai">ibm.com- brief history of IBM AI</a></li>
        <li><a href="https://archive.org/details/DTIC_ADA458918/page/38/mode/2up"><em>Archive.org</em> DTIC ADA458918:
            Shakey the Robot</a></li>
        <li><a
            href="https://www.scientificamerican.com/article/20-years-after-deep-blue-how-ai-has-advanced-since-conquering-chess/"><em>scientificamerican- 20 years after deep-blue</em></a>
        </li>
        <li><a href="https://deepmind.google/science/alphafold/">deepmind.google-alphafold/</a></li>
        <li><a href="https://www.britannica.com/technology/MYCIN">britannica.com-MYCIN</a></li>
      </ul>
    </section>
  </main>
  <div class="botonback botonbackcontainer"><a href="#"><img src="fotos-pag/svg/keyboard_double_arrow_up_30dp_FAEBD7_FILL0_wght400_GRAD0_opsz24.svg" alt="" class="imagenfooter"></a></div>
  <footer>
    <Address>Gracias por tu visita! Página hecha por Lucía Driussi- Alumna del curso "Desarrollador web Jr.Front End" E.F.P
      N*7,Resistencia,Chaco.</Address><img src="fotos-pag/Imagen de WhatsApp 2025-11-12 a las 18.08.23_0ec2f152.jpg" alt="" height="55px" width="80px" class="imagenfooter">
  </footer>
</body>
</html>